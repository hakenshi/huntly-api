# üï∑Ô∏è Sistema de Lead Scraping Autom√°tico

Sistema completo para coleta autom√°tica de leads da internet, integrado com o Huntly MVP.

## üìã Vis√£o Geral

O sistema de scraping permite coletar automaticamente informa√ß√µes de empresas e contatos de v√°rias fontes na internet:

- **Google Maps**: Empresas locais com informa√ß√µes de contato
- **LinkedIn**: Empresas e profissionais (limitado)
- **Sites de Empresas**: Informa√ß√µes diretas dos websites

## üöÄ Funcionalidades

### ‚úÖ Implementado

- **Scraping Multi-Fonte**: Coleta dados de m√∫ltiplas fontes simultaneamente
- **Gerenciamento de Jobs**: Sistema ass√≠ncrono para executar scraping em background
- **Rate Limiting**: Controle de velocidade para respeitar limites dos sites
- **Valida√ß√£o de Dados**: Verifica√ß√£o e limpeza autom√°tica dos dados coletados
- **Cache Inteligente**: Evita duplicatas e melhora performance
- **Indexa√ß√£o Autom√°tica**: Leads coletados s√£o automaticamente indexados para busca
- **API REST Completa**: Endpoints para gerenciar scraping via API
- **Templates Pr√©-configurados**: Configura√ß√µes prontas para diferentes ind√∫strias

### üîÑ Fontes de Dados Suportadas

| Fonte | Dados Coletados | Qualidade | Velocidade | Limita√ß√µes |
|-------|----------------|-----------|------------|------------|
| **Google Maps** | Empresa, telefone, endere√ßo, website | Alta | R√°pida | Rate limiting |
| **LinkedIn** | Empresa, ind√∫stria, funcion√°rios, descri√ß√£o | Muito Alta | Lenta | Requer autentica√ß√£o |
| **Sites de Empresas** | Email, telefone, descri√ß√£o detalhada | Vari√°vel | M√©dia | Estrutura vari√°vel |

## üõ†Ô∏è Como Usar

### 1. Via API REST

```python
# Iniciar scraping
POST /scraping/start
{
    "search_query": "restaurante pizza",
    "location": "S√£o Paulo, SP",
    "max_results": 100,
    "sources": ["google_maps"],
    "required_fields": ["company", "phone"]
}

# Acompanhar progresso
GET /scraping/jobs/{job_id}

# Listar jobs
GET /scraping/jobs
```

### 2. Via Python (Program√°tico)

```python
from src.scraping.manager import ScrapingManager
from src.scraping.models import ScrapingConfig, ScrapingSource

# Configurar scraping
config = ScrapingConfig(
    search_query="empresa software",
    location="S√£o Paulo",
    max_results=50,
    sources=[ScrapingSource.GOOGLE_MAPS]
)

# Executar
manager = ScrapingManager(db_session, cache_manager)
job = await manager.start_scraping_job(user_id=1, config=config)
```

### 3. Templates Prontos

```python
# Usar template pr√©-configurado
GET /scraping/templates

# Exemplo de resposta:
{
    "name": "Empresas de Tecnologia",
    "config": {
        "search_query": "empresa software desenvolvimento",
        "industry": "Tecnologia",
        "sources": ["google_maps", "linkedin"],
        "max_results": 100
    }
}
```

## ‚öôÔ∏è Configura√ß√£o

### Par√¢metros Principais

```python
ScrapingConfig(
    # Busca
    search_query="termo de busca",      # Obrigat√≥rio
    location="cidade, estado",          # Opcional
    industry="ind√∫stria",               # Opcional
    
    # Limites
    max_results=100,                    # M√°ximo de leads
    max_pages=10,                       # M√°ximo de p√°ginas
    
    # Performance
    delay_between_requests=1.0,         # Delay entre requests (segundos)
    
    # Filtros
    required_fields=["company", "email"], # Campos obrigat√≥rios
    min_employees=10,                   # M√≠nimo de funcion√°rios
    max_employees=500,                  # M√°ximo de funcion√°rios
    
    # Fontes
    sources=[ScrapingSource.GOOGLE_MAPS], # Fontes a usar
    
    # Avan√ßado
    use_proxy=False,                    # Usar proxy
    respect_robots_txt=True             # Respeitar robots.txt
)
```

### Vari√°veis de Ambiente

```bash
# Rate limiting
SCRAPING_DEFAULT_DELAY=1.0
SCRAPING_MAX_CONCURRENT=5

# Timeouts
SCRAPING_REQUEST_TIMEOUT=30
SCRAPING_SESSION_TIMEOUT=300

# Proxy (opcional)
SCRAPING_PROXY_URL=http://proxy:8080
SCRAPING_PROXY_ROTATION=true
```

## üìä Monitoramento

### Status do Job

```python
job_status = {
    "id": "uuid",
    "status": "running",           # pending, running, completed, failed
    "leads_found": 45,            # Total encontrado
    "leads_saved": 42,            # Total salvo no banco
    "progress": 75,               # Progresso %
    "estimated_completion": "2024-01-15T10:30:00Z",
    "errors": [],                 # Lista de erros
    "warnings": []                # Lista de avisos
}
```

### Estat√≠sticas Gerais

```python
GET /scraping/stats
{
    "active_jobs": 3,
    "total_leads_found": 1250,
    "total_leads_saved": 1180,
    "average_success_rate": 94.4,
    "sources_usage": {
        "google_maps": 15,
        "linkedin": 8,
        "company_website": 5
    }
}
```

## üîß Exemplos de Uso

### Restaurantes Locais

```python
config = ScrapingConfig(
    search_query="restaurante pizza delivery",
    location="Rio de Janeiro, RJ",
    sources=[ScrapingSource.GOOGLE_MAPS],
    max_results=200,
    required_fields=["company", "phone"]
)
```

### Startups de Tecnologia

```python
config = ScrapingConfig(
    search_query="startup software saas",
    industry="Tecnologia",
    sources=[ScrapingSource.LINKEDIN, ScrapingSource.COMPANY_WEBSITE],
    max_results=100,
    min_employees=5,
    max_employees=50,
    required_fields=["company", "website"]
)
```

### Servi√ßos Profissionais

```python
config = ScrapingConfig(
    search_query="advogado contador consultor",
    location="S√£o Paulo",
    sources=[ScrapingSource.GOOGLE_MAPS],
    max_results=150,
    required_fields=["company", "phone"]
)
```

## üö® Considera√ß√µes √âticas e Legais

### ‚úÖ Boas Pr√°ticas

- **Respeite robots.txt**: Sempre verificar e respeitar as diretrizes
- **Rate Limiting**: N√£o sobrecarregar os servidores
- **Dados P√∫blicos**: Coletar apenas informa√ß√µes publicamente dispon√≠veis
- **Uso Respons√°vel**: Usar dados coletados de forma √©tica

### ‚ö†Ô∏è Limita√ß√µes

- **LinkedIn**: Requer autentica√ß√£o, considere usar API oficial
- **Google Maps**: Sujeito a rate limiting e poss√≠vel bloqueio
- **Sites Empresariais**: Estrutura vari√°vel, dados podem ser incompletos

### üìã Recomenda√ß√µes

1. **Para LinkedIn**: Use a API oficial do Sales Navigator
2. **Para Google**: Considere a Google Places API
3. **Para dados espec√≠ficos**: APIs especializadas como Clearbit, ZoomInfo
4. **Para compliance**: Implemente LGPD/GDPR se necess√°rio

## üîÑ Integra√ß√£o com o Sistema

### Fluxo Completo

1. **Configura√ß√£o**: Usu√°rio define par√¢metros de busca
2. **Execu√ß√£o**: Sistema executa scraping em background
3. **Valida√ß√£o**: Dados s√£o validados e limpos
4. **Armazenamento**: Leads salvos no banco PostgreSQL
5. **Indexa√ß√£o**: Leads indexados para busca (Redis + PostgreSQL)
6. **Disponibiliza√ß√£o**: Leads dispon√≠veis via API de busca

### Integra√ß√£o com Search Engine

```python
# Leads coletados s√£o automaticamente indexados
scraping_manager = ScrapingManager(db, cache)
job = await scraping_manager.start_scraping_job(user_id, config)

# Ap√≥s conclus√£o, leads est√£o dispon√≠veis para busca
search_engine = SearchEngine(db, cache)
results = search_engine.search_leads(SearchQuery(text="pizza delivery"))
```

## üöÄ Pr√≥ximos Passos

### Melhorias Planejadas

- [ ] **Scraping de Redes Sociais**: Instagram, Facebook Business
- [ ] **APIs Especializadas**: Integra√ß√£o com Clearbit, ZoomInfo
- [ ] **Machine Learning**: Classifica√ß√£o autom√°tica de ind√∫strias
- [ ] **Enriquecimento de Dados**: Valida√ß√£o de emails, telefones
- [ ] **Scraping Agendado**: Jobs recorrentes autom√°ticos
- [ ] **Dashboard Visual**: Interface para monitoramento
- [ ] **Exporta√ß√£o**: CSV, Excel, integra√ß√£o com CRMs

### Otimiza√ß√µes

- [ ] **Proxy Rotation**: Rota√ß√£o autom√°tica de proxies
- [ ] **CAPTCHA Solving**: Integra√ß√£o com servi√ßos de CAPTCHA
- [ ] **Selenium Integration**: Para sites com JavaScript pesado
- [ ] **Distributed Scraping**: Scraping distribu√≠do para escala

## üìö Documenta√ß√£o Adicional

- [API Reference](../routes/scraping.py) - Documenta√ß√£o completa da API
- [Examples](example_usage.py) - Exemplos pr√°ticos de uso
- [Models](models.py) - Modelos de dados detalhados
- [Scrapers](scrapers/) - Implementa√ß√£o dos scrapers individuais

---

**‚ö° Dica**: Para come√ßar rapidamente, use os templates pr√©-configurados dispon√≠veis em `/scraping/templates`!